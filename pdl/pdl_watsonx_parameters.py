# generated by datamodel-codegen:
#   filename:  watsonx-ai.json
#   timestamp: 2024-08-09T17:13:43+00:00

from __future__ import annotations

from datetime import date, datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Set

from pydantic import BaseModel, Extra, Field, PositiveInt, confloat, conint, constr

PDLVariable: TypeAlias = str

class TextGenCommon(BaseModel):
    """
    A prompt to be used to infer the next tokens. Each request expects a `project_id` or a `space_id`, and the `project` or `space` must have an associated WML instance that will be used for limits and billing (if a paid plan).

    """

    input: str
    """
    The prompt to generate completions.
    Note: The method tokenizes the input internally.
    It is recommended not to leave any trailing spaces.

    """


class TextGenLengthPenalty(BaseModel):
    """
    It can be used to exponentially increase the likelihood of the text generation terminating once a specified number of tokens have been generated.

    """

    decay_factor: Optional[PDLVariable | confloat(gt=1.0)] = Field(default=None, example=2.5)
    """
    Represents the factor of exponential decay.
    Larger values correspond to more aggressive decay.

    """
    start_index: Optional[PDLVariable | conint(ge=0)] = Field(default=None, example=5)
    """
    A number of generated tokens after which this should take effect.

    """


class ReturnOptionProperties(BaseModel):
    """
    Properties that control what is returned.

    """

    input_text: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    Include input text in the `generated_text` field.

    """
    generated_tokens: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    Include the list of individual generated tokens.
    Extra token information is included based on the other flags below.

    """
    input_tokens: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    Include the list of input tokens. Extra token information is included based
    on the other flags here, but only for decoder-only models.

    """
    token_logprobs: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    Include logprob (natural log of probability) for each returned token.
    Applicable only if generated_tokens == true and/or input_tokens == true.

    """
    token_ranks: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    Include rank of each returned token.
    Applicable only if generated_tokens == true and/or input_tokens == true.

    """
    top_n_tokens: Optional[PDLVariable | conint(ge=0)] = Field(default=None, example=2)
    """
    Include top n candidate tokens at the position of each returned token.
    The maximum value permitted is 5, but more may be returned if there is a tie for nth place.
    Applicable only if generated_tokens == true and/or input_tokens == true.

    """


class DecodingMethod(Enum):
    """
    Represents the strategy used for picking the tokens during generation of the output text.

    During text generation when parameter value is set to greedy, each successive token corresponds
    to the highest probability token given the text that has already been generated.
    This strategy can lead to repetitive results especially for longer output sequences.
    The alternative sample strategy generates text by picking subsequent tokens based on the
    probability distribution of possible next tokens defined by (i.e., conditioned on) the
    already-generated text and the top_k and top_p parameters described below.
    See this [url](https://huggingface.co/blog/how-to-generate) for an informative article about text generation.

    """

    sample = 'sample'
    greedy = 'greedy'


class TextGenParameters(BaseModel):
    """
    Properties that control the model and response.

    """

    decoding_method: Optional[PDLVariable | DecodingMethod] = Field(
        default='sample', example='greedy'
    )
    """
    Represents the strategy used for picking the tokens during generation of the output text.

    During text generation when parameter value is set to greedy, each successive token corresponds
    to the highest probability token given the text that has already been generated.
    This strategy can lead to repetitive results especially for longer output sequences.
    The alternative sample strategy generates text by picking subsequent tokens based on the
    probability distribution of possible next tokens defined by (i.e., conditioned on) the
    already-generated text and the top_k and top_p parameters described below.
    See this [url](https://huggingface.co/blog/how-to-generate) for an informative article about text generation.

    """
    length_penalty: Optional[PDLVariable | TextGenLengthPenalty] = None
    max_new_tokens: Optional[PDLVariable | conint(ge=0)] = Field(default=20, example=30)
    """
    The maximum number of new tokens to be generated.
    The maximum supported value for this field depends on the model being used.

    How the "token" is defined depends on the tokenizer and vocabulary size,
    which in turn depends on the model. Often the tokens are a mix of full words and sub-words.
    To learn more about tokenization, [see here](https://huggingface.co/course/chapter2/4).

    Depending on the users plan, and on the model being used, there may be an enforced maximum number of new tokens.

    """
    min_new_tokens: Optional[PDLVariable | conint(ge=0)] = Field(default=0, example=5)
    """
    If stop sequences are given, they are ignored until minimum tokens are generated.

    """
    random_seed: Optional[PDLVariable | conint(ge=1)] = Field(default=None, example=1)
    """
    Random number generator seed to use in sampling mode for experimental repeatability.

    """
    stop_sequences: Optional[PDLVariable | Set[str]] = Field(
        default=None, example=['fail'], max_items=6, min_items=0,
    )
    """
    Stop sequences are one or more strings which will cause the text generation to stop if/when they are produced as part of the output.
    Stop sequences encountered prior to the minimum number of tokens being generated will be ignored.

    """
    temperature: Optional[PDLVariable | confloat(ge=0.0, le=2.0)] = Field(default=1, example=1.5)
    """
    A value used to modify the next-token probabilities in sampling mode.
    Values less than 1.0 sharpen the probability distribution, resulting in "less random" output.
    Values greater than 1.0 flatten the probability distribution, resulting in "more random" output.
    A value of 1.0 has no effect.

    """
    time_limit: Optional[PDLVariable | PositiveInt] = Field(default=None, example=600000)
    """
    Time limit in milliseconds - if not completed within this time, generation will stop.
    The text generated so far will be returned along with the TIME_LIMIT stop reason.

    Depending on the users plan, and on the model being used, there may be an enforced maximum time limit.

    """
    top_k: Optional[PDLVariable | conint(ge=1, le=100)] = Field(default=None, example=50)
    """
    The number of highest probability vocabulary tokens to keep for top-k-filtering.
    Only applies for sampling mode. When decoding_strategy is set to sample,
    only the top_k most likely tokens are considered as candidates for the next generated token.

    """
    top_p: Optional[PDLVariable | confloat(le=1.0, gt=0.0)] = Field(default=1, example=0.5)
    """
    Similar to top_k except the candidates to generate the next token are the most likely tokens
    with probabilities that add up to at least top_p. Also known as nucleus sampling.
    A value of 1.0 is equivalent to disabled.

    """
    repetition_penalty: Optional[PDLVariable | confloat(ge=1.0, le=2.0)] = Field(
        default=1, example=1.5
    )
    """
    Represents the penalty for penalizing tokens that have already been generated or
    belong to the context. The value 1.0 means that there is no penalty.

    """
    truncate_input_tokens: Optional[PDLVariable | conint(ge=1)] = None
    """
    Represents the maximum number of input tokens accepted.
    This can be used to avoid requests failing due to input being longer than configured limits. If the text is truncated, then it truncates the start of the input (on the left), so the end of the input will remain the same.
    If this value exceeds the `maximum sequence length` (refer to the documentation to find this value for the model) then the call will fail if the total number of tokens exceeds the `maximum sequence length`.

    """
    return_options: Optional[PDLVariable | ReturnOptionProperties] = None
    include_stop_sequence: Optional[PDLVariable | bool] = True
    """
    Pass `false` to omit matched stop sequences from the end of the output text.
    The default is `true`, meaning that the output will end with the stop sequence text when matched.

    """


class TextModeration(BaseModel):
    """
    Properties that control the moderation on the text.

    """

    enabled: Optional[PDLVariable | bool] = True
    """
    Should this moderation be enabled on the text.


    The default value is `true` which means that if the parent object exists
    but the `enabled` field does not exist then this is considered to be enabled.

    """
    threshold: Optional[PDLVariable | confloat(ge=0.0, le=1.0)] = None
    """
    The threshold probability that this is a real match.

    """


class ModerationProperties(BaseModel):
    """
    The properties for the moderation. Each type of moderation
    may have additional properties that are specific to that moderation.

    """

    input: Optional[PDLVariable | TextModeration] = None
    output: Optional[PDLVariable | TextModeration] = None


class MaskProperties(BaseModel):
    """
    The properties specific to masking. If this object exists,
    even if it is empty, then masking will be applied.

    """

    remove_entity_value: Optional[PDLVariable | bool] = False
    """
    If this field is `true` then the entity value, that contains the text that was masked,
    will also be removed from the output.

    """


class HapProperties(BaseModel):
    """
    The properties specific to HAP.

    """

    mask: Optional[PDLVariable | MaskProperties] = None


class ModerationHapProperties(ModerationProperties, HapProperties):
    pass


class PiiProperties(BaseModel):
    """
    The properties specific to PII.

    """

    mask: Optional[PDLVariable | MaskProperties] = None


class ModerationPiiProperties(ModerationProperties, PiiProperties):
    pass


class ModerationTextRange(BaseModel):
    """
    A range of text.

    """

    start: conint(ge=0)
    """
    The start index of the range.

    """
    end: conint(ge=0)
    """
    The end index of the range. The end index is exclusive meaning that the character at this index will not be included in the range.

    """


class Moderations(BaseModel):
    """
    Properties that control the moderations, for usages such as `Hate and profanity` (HAP) and `Personal identifiable information` (PII) filtering. This list can be extended with new types of moderations.

    """

    hap: Optional[PDLVariable | ModerationHapProperties] = None
    pii: Optional[PDLVariable | ModerationPiiProperties] = None
    input_ranges: Optional[PDLVariable | List[ModerationTextRange]] = None
    """
    If set, then only these ranges will be applied to the moderations. This is useful in the case that certain parts of the input text have already been checked.

    """


class TextGenRequest(TextGenCommon):
    model_id: str = Field(..., example='google/flan-ul2')
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx).

    """
    space_id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)] = (
        Field(default=None, example='3fc54cf1-252f-424b-b52d-5cdd9814987f')
    )
    """
    The space that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    project_id: Optional[PDLVariable | 
            constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)
        ] = Field(default=None, example='12ac4cf1-252f-424b-b52d-5cdd9814987f')
    """
    The project that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    parameters: Optional[PDLVariable | TextGenParameters] = None
    moderations: Optional[PDLVariable | Moderations] = None


class TextGenStopReason(Enum):
    """
    The reason why the call stopped, can be one of:
    - not_finished - Possibly more tokens to be streamed.
    - max_tokens - Maximum requested tokens reached.
    - eos_token - End of sequence token encountered.
    - cancelled - Request canceled by the client.
    - time_limit - Time limit reached.
    - stop_sequence - Stop sequence encountered.
    - token_limit - Token limit reached.
    - error - Error encountered.

    Note that these values will be lower-cased so test for values case insensitive.

    """

    not_finished = 'not_finished'
    max_tokens = 'max_tokens'
    eos_token = 'eos_token'
    cancelled = 'cancelled'
    time_limit = 'time_limit'
    stop_sequence = 'stop_sequence'
    token_limit = 'token_limit'
    error = 'error'


class TextGenResult(BaseModel):
    generated_text: str = Field(..., example='Swimwear Unlimited- Mid-Summer Sale! ...')
    """
    The text that was generated by the model.

    """
    stop_reason: TextGenStopReason


class TextGenTopTokenInfo(BaseModel):
    """
    The top tokens.

    """

    text: Optional[PDLVariable | str] = None
    """
    The token text.

    """
    logprob: Optional[PDLVariable | float] = None
    """
    The natural log of probability for the token.

    """


class TextGenTokenInfo(BaseModel):
    """
    The generated token.

    """

    text: Optional[PDLVariable | str] = None
    """
    The token text.

    """
    logprob: Optional[PDLVariable | float] = None
    """
    The natural log of probability for the token.

    """
    rank: Optional[PDLVariable | int] = None
    """
    The rank of the token relative to the other tokens.

    """
    top_tokens: Optional[PDLVariable | List[TextGenTopTokenInfo]] = Field(default=None, min_items=0)
    """
    The top tokens.

    """


class TextGenResultFields(BaseModel):
    generated_token_count: Optional[PDLVariable | int] = Field(default=None, example=3)
    """
    The number of generated tokens.

    """
    input_token_count: Optional[PDLVariable | int] = Field(default=None, example=11)
    """
    The number of input tokens consumed.

    """
    seed: Optional[PDLVariable | int] = Field(default=None, example=42)
    """
    The seed used, if it exists.

    """
    generated_tokens: Optional[PDLVariable | List[TextGenTokenInfo]] = Field(
        default=None,
        example=[
            {
                'text': '_',
                'rank': 1,
                'logprob': -2.5,
                'top_tokens': [
                    {'text': '_', 'logprob': -2.5},
                    {'text': '_2', 'logprob': -3.1777344},
                ],
            },
            {
                'text': '4,000',
                'rank': 1,
                'logprob': -3.0957031,
                'top_tokens': [
                    {'text': '4,000', 'logprob': -3.0957031},
                    {'text': '57', 'logprob': -3.3691406},
                ],
            },
        ],
        min_items=1,
    )
    """
    The list of individual generated tokens.
    Extra token information is included based on the other flags in the `return_options` of the request.

    """
    input_tokens: Optional[PDLVariable | List[TextGenTokenInfo]] = Field(
        default=None,
        example=[{'text': '_how'}, {'text': '_far'}, {'text': '_is'}, {'text': '</s>'}],
        min_items=1,
    )
    """
    The list of input tokens.
    Extra token information is included based on the other flags in the `return_options` of the request, but for decoder-only models.

    """


class Result(TextGenResult, TextGenResultFields):
    pass


class TextGenResponseFields(BaseModel):
    """
    The tokens that are inferred from the prompt.

    Note that the events are different between a request with moderation and
    a request without moderations.
    A request with moderation will have events that are on a sentence level
    and a request without moderations will have events that are on a token level.

    """

    model_id: str = Field(..., example='google/flan-ul2')
    """
    The `id` of the model for inference.

    """
    model_version: Optional[PDLVariable | 
            constr(pattern=r'^\d+.\d+.\d+$', min_length=5, max_length=20)
        ] = Field(default=None, example='1.0.1')
    """
    The model version (using semantic versioning) if set.

    """
    created_at: datetime
    """
    The time when the response was created.

    """
    results: List[PDLVariable | Result] = Field(..., min_items=1)
    """
    The generated tokens.

    """


class Warning(BaseModel):
    """
    A warning message.

    """

    message: str = Field(..., example='The framework TF 1.1 is deprecated.')
    """
    The message.

    """
    id: Optional[PDLVariable | str] = Field(
        default=None, example='2fc54cf1-252f-424b-b52d-5cdd98149871'
    )
    """
    An `id` associated with the message.

    """
    more_info: Optional[PDLVariable | str] = None
    """
    A reference to a more detailed explanation when available.

    """
    additional_properties: Optional[PDLVariable | Dict[str, Any]] = None
    """
    Additional key-value pairs that depend on the specific warning.

    """


class SystemDetails(BaseModel):
    """
    Optional details coming from the service and related to the API call or the associated resource.

    """

    warnings: Optional[PDLVariable | List[Warning]] = None
    """
    Any warnings coming from the system.

    """


class System(BaseModel):
    """
    System details.

    """

    system: Optional[PDLVariable | SystemDetails] = None


class TextGenResponse(TextGenResponseFields, System):
    pass


class Type(Enum):
    """
    The type of the problematic field.

    """

    field = 'field'
    query = 'query'
    header = 'header'


class ApiErrorTarget(BaseModel):
    """
    The target of the error.

    """

    type: Type
    """
    The type of the problematic field.

    """
    name: str
    """
    The name of the problematic field.

    """


class ApiError(BaseModel):
    """
    An error message.

    """

    code: str = Field(..., example='missing_field')
    """
    A simple code that should convey the general sense of the error.

    """
    message: str = Field(..., example="The 'name' field is required.")
    """
    The message that describes the error.

    """
    more_info: Optional[PDLVariable | str] = Field(
        default=None,
        example='https://cloud.ibm.com/apidocs/machine-learning#models-get',
    )
    """
    A reference to a more detailed explanation when available.

    """
    target: Optional[PDLVariable | ApiErrorTarget] = None


class ApiErrorResponse(BaseModel):
    """
    The data returned when an error is encountered.

    """

    trace: str = Field(..., example='3fd543d2-36e0-4f83-9be3-5c6dd498af4f')
    """
    An identifier that can be used to trace the request.

    """
    errors: List[PDLVariable | ApiError]
    """
    The list of errors.

    """


class TextGenStreamResponse(BaseModel):
    """
    A set of server sent events, each event contains a response for one or more tokens. The results will be an array of events of the form `data: {<json event>}` where the schema of the individual `json event` is described below.

    """

    __root__: List[PDLVariable | TextGenResponse]
    """
    A set of server sent events, each event contains a response for one or more tokens. The results will be an array of events of the form `data: {<json event>}` where the schema of the individual `json event` is described below.

    """


class TextTokenizeParameters(BaseModel):
    """
    The parameters for text tokenization.

    """

    return_tokens: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    If this is `true` then the actual tokens will also be returned in the response.

    """


class TextTokenizeRequest(BaseModel):
    """
    The input string to tokenize along with the associated model id and any parameters.
    One of `space_id` or `project_id` must be provided.

    """

    space_id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)] = (
        Field(default=None, example='3fc54cf1-252f-424b-b52d-5cdd9814987f')
    )
    """
    The space that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    project_id: Optional[PDLVariable | 
            constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)
        ] = Field(default=None, example='12ac4cf1-252f-424b-b52d-5cdd9814987f')
    """
    The project that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    model_id: str = Field(..., example='google/flan-ul2')
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx).

    """
    input: str = Field(
        ..., example='Write a tagline for an alumni association: Together we'
    )
    """
    The input string to tokenize.

    """
    parameters: Optional[PDLVariable | TextTokenizeParameters] = None


class TextTokenizeResult(BaseModel):
    """
    The result of tokenizing the input string.

    """

    token_count: int = Field(..., example=11)
    """
    The number of tokens in the input string.

    """
    tokens: Optional[PDLVariable | List[str]] = Field(
        default=None,
        example=[
            'Write',
            'a',
            'tag',
            'line',
            'for',
            'an',
            'alumni',
            'associ',
            'ation:',
            'Together',
            'we',
        ],
    )
    """
    The input string broken up into the tokens, if requested.

    """


class TextTokenizeResponse(BaseModel):
    """
    The tokenization result.

    """

    model_id: str = Field(..., example='google/flan-ul2')
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx).

    """
    result: TextTokenizeResult


class PaginationFirst(BaseModel):
    """
    The reference to the first item in the current page.

    """

    href: str
    """
    The uri of the first resource returned.

    """


class PaginationNext(BaseModel):
    """
    A reference to the first item of the next page, if any.

    """

    href: str
    """
    The uri of the next set of resources.

    """


class PaginationBase(BaseModel):
    total_count: Optional[PDLVariable | int] = None
    """
    Computed explicitly only when 'total_count=true' query parameter is present.
    This is in order to avoid performance penalties.

    """
    limit: conint(ge=1, le=200) = Field(..., example=10)
    """
    The number of items to return in each page.

    """
    first: PaginationFirst
    next: Optional[PDLVariable | PaginationNext] = None


class PaginationTC(PaginationBase):
    """
    Information for paging when querying resources.

    """

    total_count: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The total number of resources.

    """


class ConsumptionsLimit(BaseModel):
    """
    The limits that may be set per request.

    """

    call_time: Optional[PDLVariable | str] = Field(default=None, example='3S')
    """
    The hard limit on the call time for a request, if set.

    """
    max_input_tokens: Optional[PDLVariable | int] = Field(default=None, example=200)
    """
    The hard limit on the number of input tokens for a request, if set.
    A value of zero will disable this feature.

    """
    max_output_tokens: Optional[PDLVariable | int] = Field(default=None, example=1000)
    """
    The hard limit on the number of output tokens for a request, if set.
    A value of zero will disable this feature.

    """


class FoundationModelLimits(BaseModel):
    """
    Limits per plan that may be set per request.

    """

    lite: Optional[PDLVariable | ConsumptionsLimit] = None


class TaskRating(BaseModel):
    """
    The ratings for this task for this model.

    """

    cost: Optional[PDLVariable | conint(ge=1, le=5)] = Field(default=None, example=2)
    """
    A metric that indicates the cost expected to be incurred by the model's support of an inference task,
    in terms of resource consumption and processing time,
    on a scale of 1 to 5, where 5 is the least cost and 1 is the most cost.
    A missing value means that the cost is not known.

    """
    quality: Optional[PDLVariable | conint(ge=1, le=5)] = Field(default=None, example=3)
    """
    A metric that indicates the quality of the model's support of an inference task,
    on a scale of 1 to 5, where 5 is the best support and 1 is poor support.
    A missing value means that the quality is not known.

    """


class TaskDescription(BaseModel):
    """
    The attributes of the task for this model.

    """

    id: str = Field(..., example='summarization')
    """
    The `id` of the task.

    """
    ratings: Optional[PDLVariable | TaskRating] = None
    tags: Optional[PDLVariable | List[str]] = None
    """
    The tags for a given task.

    """


class FoundationModelTier(Enum):
    """
    The tier of the model, depending on the `tier` the billing will be different, refer to the plan for the details. Note that input tokens and output tokens may be charged differently.

    """

    class_1 = 'class_1'
    class_2 = 'class_2'
    class_3 = 'class_3'
    class_c1 = 'class_c1'


class ModelLimits(BaseModel):
    """
    The limits that are applied for the model, for all the plans.

    """

    max_sequence_length: Optional[PDLVariable | int] = Field(default=None, example=4096)
    """
    This is the maximum allowed value for the number of tokens in the input
    prompt plus the number of tokens in the output generated by the model.

    """
    training_data_max_records: Optional[PDLVariable | int] = Field(default=None, example=1024)
    """
    This is the maximum number of records that can be accepted when training this model.

    """


class Id(Enum):
    """
    The possible lifecycle stages, in order, are described below:

    - `available`: this means that the model is available for use.
    - `deprecated`: this means that the model is still available but the model will be removed soon, so an alternative model should be used.
    - `constricted`: this means that the model is still available for inferencing but cannot be used for training or in a deployment. The model will be removed soon so an alternative model should be used.
    - `withdrawn`: this means that the model is no longer available, check the `alternative_model_ids` to see what it can be replaced by.

    """

    available = 'available'
    deprecated = 'deprecated'
    constricted = 'constricted'
    withdrawn = 'withdrawn'


class LifeCycleState(BaseModel):
    """
    The lifecycle details.

    """

    id: Id = Field(..., example='available')
    """
    The possible lifecycle stages, in order, are described below:

    - `available`: this means that the model is available for use.
    - `deprecated`: this means that the model is still available but the model will be removed soon, so an alternative model should be used.
    - `constricted`: this means that the model is still available for inferencing but cannot be used for training or in a deployment. The model will be removed soon so an alternative model should be used.
    - `withdrawn`: this means that the model is no longer available, check the `alternative_model_ids` to see what it can be replaced by.

    """
    label: Optional[PDLVariable | str] = None
    """
    An optional label that may be used in the UI.

    """
    start_date: Optional[PDLVariable | date] = Field(default=None, example='2023-07-23')
    """
    The date (ISO 8601 format YYYY-MM-DD) when this lifecycle stage starts.

    """
    alternative_model_ids: Optional[PDLVariable | List[str]] = None
    """
    Alternative models, or model versions, that can be used instead of this model.

    """
    url: Optional[PDLVariable | str] = None
    """
    A link to the documentation specifying details on the lifecycle plan for this model.

    """


class TrainingInitMethod(BaseModel):
    """
    Initialization methods for a training.

    """

    supported: Optional[PDLVariable | List[str]] = Field(default=None, example=['random', 'text'])
    """
    The supported initialization methods.

    """
    default: Optional[PDLVariable | str] = Field(default=None, example='random')
    """
    The default value, which will be one of the values from the `supported` field.

    """


class TrainingInitText(BaseModel):
    """
    Initialization text to be used if init_method is set to `text`, otherwise this will be ignored.

    """

    default: Optional[PDLVariable | str] = Field(default=None, example='text')
    """
    Initialization text.

    """


class TrainingNumVirtualTokens(BaseModel):
    """
    Number of virtual tokens to be used for training.
    In prompt tuning we are essentially learning the embedded representations for soft prompts,
    which are known as virtual tokens, via back propagation for a specific task(s) while keeping
    the rest of the model fixed. `num_virtual_tokens` is the number of dimensions for these virtual tokens.

    """

    supported: Optional[PDLVariable | List[int]] = Field(default=None, example=[20, 50, 100])
    """
    The possible values for the number of virtual tokens.

    """
    default: Optional[PDLVariable | int] = Field(default=None, example=100)
    """
    The default number of virtual tokens.

    """


class TrainingNumEpochs(BaseModel):
    """
    The number of epochs is the number of complete passes through the training dataset.
    The quality depends on the number of epochs.

    """

    default: Optional[PDLVariable | int] = Field(default=None, example=20)
    """
    The default value.

    """
    min: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The minimum value.

    """
    max: Optional[PDLVariable | int] = Field(default=None, example=50)
    """
    The maximum value.

    """


class TrainingVerbalizer(BaseModel):
    """
    Verbalizer template to be used for formatting data at train and inference time.
    This template may use brackets to indicate where fields from the data model
    TrainGenerationRecord must be rendered.

    """

    default: Optional[PDLVariable | str] = Field(default=None, example='Input: {{input}} Output:')
    """
    The default verbalizer.

    """


class TrainingBatchSize(BaseModel):
    """
    The batch size is a number of samples processed before the model is updated.

    """

    default: Optional[PDLVariable | int] = Field(default=None, example=16)
    """
    The default value.

    """
    min: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The minimum value.

    """
    max: Optional[PDLVariable | int] = Field(default=None, example=16)
    """
    The maximum value.

    """


class TrainingMaxInputTokens(BaseModel):
    """
    Maximum length of input tokens being considered.

    """

    default: Optional[PDLVariable | int] = Field(default=None, example=256)
    """
    The default value.

    """
    min: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The minimum value.

    """
    max: Optional[PDLVariable | int] = Field(default=None, example=1024)
    """
    The maximum value.

    """


class TrainingMaxOutputTokens(BaseModel):
    """
    Maximum length of output tokens being predicted.

    """

    default: Optional[PDLVariable | int] = Field(default=None, example=128)
    """
    The default value.

    """
    min: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The minimum value.

    """
    max: Optional[PDLVariable | int] = Field(default=None, example=256)
    """
    The maximum value.

    """


class TrainingTorchDtype(BaseModel):
    """
    Datatype to use for training of the underlying text generation model.
    If no value is provided, we pull from torch_dtype in config.
    If an in memory resource is provided which does not match the specified data type,
    the model underpinning the resource will be converted in place to the correct torch dtype.

    """

    default: Optional[PDLVariable | str] = Field(default=None, example='bfloat16')
    """
    The datatype.

    """


class TrainingAccumulatedSteps(BaseModel):
    """
    Number of steps to be used for gradient accumulation.
    Gradient accumulation refers to a method of collecting gradient for configured number of steps
    instead of updating the model variables at every step and then applying the update to model variables.
    This can be used as a tool to overcome smaller batch size limitation.
    Often also referred in conjunction with "effective batch size".

    """

    default: Optional[PDLVariable | int] = Field(default=None, example=128)
    """
    The default value.

    """
    min: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The minimum value.

    """
    max: Optional[PDLVariable | int] = Field(default=None, example=128)
    """
    The maximum value.

    """


class TrainingLearningRate(BaseModel):
    """
    Learning rate to be used for training.

    """

    default: Optional[PDLVariable | float] = Field(default=None, example=0.3)
    """
    The default value.

    """
    min: Optional[PDLVariable | float] = Field(default=None, example=0.01)
    """
    The minimum value.

    """
    max: Optional[PDLVariable | float] = Field(default=None, example=0.5)
    """
    The maximum value.

    """


class TrainingParameters(BaseModel):
    """
    Training parameters for a given model.

    """

    init_method: Optional[PDLVariable | TrainingInitMethod] = None
    init_text: Optional[PDLVariable | TrainingInitText] = None
    num_virtual_tokens: Optional[PDLVariable | TrainingNumVirtualTokens] = None
    num_epochs: Optional[PDLVariable | TrainingNumEpochs] = None
    verbalizer: Optional[PDLVariable | TrainingVerbalizer] = None
    batch_size: Optional[PDLVariable | TrainingBatchSize] = None
    max_input_tokens: Optional[PDLVariable | TrainingMaxInputTokens] = None
    max_output_tokens: Optional[PDLVariable | TrainingMaxOutputTokens] = None
    torch_dtype: Optional[PDLVariable | TrainingTorchDtype] = None
    accumulate_steps: Optional[PDLVariable | TrainingAccumulatedSteps] = None
    learning_rate: Optional[PDLVariable | TrainingLearningRate] = None


class FoundationModelVersion(BaseModel):
    """
    A minor or patch version for the model.

    """

    version: Optional[PDLVariable | constr(pattern=r'^\d+.\d+.\d+$', min_length=5, max_length=20)] = (
        Field(default=None, example='1.1.0')
    )
    """
    The version of the model. This must follow semantic versioning semantics.

    """
    available_date: Optional[PDLVariable | date] = Field(default=None, example='2023-08-23')
    """
    The date (ISO 8601 format YYYY-MM-DD) when this version first became available.

    """


class FoundationModel(BaseModel):
    """
    A supported foundation model.

    """

    model_id: str = Field(..., example='google/flan-ul2')
    """
    The id of the foundation model.

    """
    label: str = Field(..., example='flan-ul2 (20B)')
    """
    A short label that will be displayed in the UI.

    """
    provider: str = Field(..., example='Hugging Face')
    """
    The provider of the model.

    """
    tuned_by: Optional[PDLVariable | str] = None
    """
    The organization or person that tuned this model.

    """
    short_description: str = Field(
        ...,
        example='An encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned LAnguage Net.',
    )
    """
    A short description of the model suitable for a title.

    """
    long_description: Optional[PDLVariable | str] = Field(
        default=None,
        example='flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned LAnguage Net (FLAN).',
    )
    """
    A longer description of the model, that may be used if no `description_url` is provided.

    """
    limits: Optional[PDLVariable | FoundationModelLimits] = None
    task_ids: Optional[PDLVariable | List[str]] = None
    """
    Deprecated: please use `tasks` instead.

    """
    tasks: Optional[PDLVariable | List[TaskDescription]] = Field(
        default=None,
        example=[
            {'id': 'summarization', 'ratings': {'cost': 2, 'quality': 3}},
            {'id': 'classification', 'ratings': {'cost': 4, 'quality': 2}},
        ],
        min_items=1,
    )
    """
    The tasks that are supported by this model.

    """
    input_tier: FoundationModelTier
    output_tier: FoundationModelTier
    source: str = Field(..., example='Hugging Face')
    """
    Specifies the provider of this model.

    """
    min_shot_size: Optional[PDLVariable | conint(ge=0)] = Field(default=None, example=10)
    """
    The minimum number of examples required for the model.

    """
    number_params: str = Field(..., example='20b')
    """
    The number of parameters used for the model,
    it will accept `m` for million, `b` for billion and `t` for trillion.

    """
    model_limits: Optional[PDLVariable | ModelLimits] = None
    lifecycle: Optional[PDLVariable | List[LifeCycleState]] = Field(default=None, min_items=0)
    """
    The information related to the lifecycle of this model.

    """
    training_parameters: Optional[PDLVariable | TrainingParameters] = None
    versions: Optional[PDLVariable | List[FoundationModelVersion]] = Field(default=None, min_items=0)
    """
    The information related to the minor versions of this model.

    """


class FoundationModels(PaginationTC, System):
    resources: Optional[PDLVariable | List[FoundationModel]] = None
    """
    The supported foundation models.

    """


class FoundationModelTask(BaseModel):
    """
    A task that is covered by some of the foundation models that are supported in the service.

    """

    task_id: str = Field(..., example='summarization')
    """
    The id of the task.

    """
    label: str = Field(..., example='Summarization')
    """
    The label of the task.

    """
    description: Optional[PDLVariable | str] = Field(
        default=None,
        example='Models that are able to summarize documents based on some criteria.',
    )
    """
    The description of the task.

    """
    rank: int = Field(..., example=1)
    """
    The rank of the task that is mainly for the UI.

    """


class FoundationModelTasks(PaginationTC, System):
    resources: Optional[PDLVariable | List[FoundationModelTask]] = None
    """
    The supported foundation model tasks.

    """


class Pagination(PaginationBase):
    """
    Information for paging when querying resources.

    """

    total_count: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The total number of resources.
    Computed explicitly only when 'total_count=true' query parameter is present.
    This is in order to avoid performance penalties.

    """


class ResourceMetaSimple(BaseModel):
    """
    Common metadata for a simple resource.

    """

    id: str
    """
    The id of the resource.

    """
    created_at: datetime
    """
    The time when the resource was created.

    """


class ResourceCommitInfo(BaseModel):
    """
    Information related to the revision.

    """

    committed_at: datetime
    """
    The time when the revision was committed.

    """
    commit_message: Optional[PDLVariable | str] = None
    """
    The message that was provided when the revision was created.

    """


class ResourceMetaBase(BaseModel):
    """
    Common metadata for a resource.

    """

    rev: Optional[PDLVariable | str] = None
    """
    The revision of the resource.

    """
    owner: Optional[PDLVariable | str] = None
    """
    The user id which created this resource.

    """
    modified_at: Optional[PDLVariable | datetime] = None
    """
    The time when the resource was last modified.

    """
    parent_id: Optional[PDLVariable | str] = None
    """
    The id of the parent resource where applicable.

    """
    name: Optional[PDLVariable | str] = None
    """
    The name of the resource.

    """
    description: Optional[PDLVariable | str] = None
    """
    A description of the resource.

    """
    tags: Optional[PDLVariable | List[str]] = Field(default=None, example=['t1', 't2'])
    """
    A list of tags for this resource.

    """
    commit_info: Optional[PDLVariable | ResourceCommitInfo] = None


class ResourceMeta(ResourceMetaSimple, ResourceMetaBase):
    space_id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)] = (
        Field(default=None, example='3fc54cf1-252f-424b-b52d-5cdd9814987f')
    )
    """
    The space that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    project_id: Optional[PDLVariable | 
            constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)
        ] = Field(default=None, example='12ac4cf1-252f-424b-b52d-5cdd9814987f')
    """
    The project that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """


class Custom(BaseModel):
    """
    User defined properties specified as key-value pairs.

    """

    class Config:
        extra = Extra.allow


class SimpleRel(BaseModel):
    """
    A reference to a resource.

    """

    id: str = Field(..., example='4cedab6d-e8e4-4214-b81a-2ddb122db2ab')
    """
    The id of the referenced resource.

    """


class OnlineDeploymentParameters(BaseModel):
    """
    A set of key-value pairs that are used to configure the deployment.

    """

    serving_name: Optional[PDLVariable | 
            constr(pattern=r'^[a-z,0-9,_]+$', min_length=3, max_length=36)
        ] = Field(default=None, example='churn')
    """
    The `serving_name` can be used in the inference URL in place of the `deployment_id`.

    """


class OnlineDeployment(BaseModel):
    """
    Indicates that this is an online deployment. An object has to be specified but can be empty.
    The `serving_name` can be provided in the `online.parameters`.

    """

    parameters: Optional[PDLVariable | OnlineDeploymentParameters] = None


class HardwareSpec(BaseModel):
    """
    A hardware specification.

    """

    id: Optional[PDLVariable | str] = Field(
        default=None, example='4cedab6d-e8e4-4214-b81a-2ddb122db2ab'
    )
    """
    The id of the hardware specification.

    """
    rev: Optional[PDLVariable | str] = Field(default=None, example='2')
    """
    The revision of the hardware specification.

    """
    name: Optional[PDLVariable | str] = None
    """
    The name of the hardware specification.

    """
    num_nodes: Optional[PDLVariable | int] = Field(default=None, example=2)
    """
    The number of nodes applied to a computation.

    """


class Size(Enum):
    """
    The size of GPU requested for the deployment.

    """

    gpu_s = 'gpu_s'
    gpu_m = 'gpu_m'


class HardwareRequest(BaseModel):
    """
    The requested hardware for deployment.

    """

    size: Optional[PDLVariable | Size] = None
    """
    The size of GPU requested for the deployment.

    """
    num_nodes: Optional[PDLVariable | float] = None
    """
    The number of nodes for the GPU requested for deployment.

    """


class DeploymentEntityCommon(BaseModel):
    """
    The common fields between a request and the response.

    """

    custom: Optional[PDLVariable | Custom] = None
    prompt_template: Optional[PDLVariable | SimpleRel] = None
    online: OnlineDeployment
    hardware_spec: Optional[PDLVariable | HardwareSpec] = None
    hardware_request: Optional[PDLVariable | HardwareRequest] = None


class ModelRel(SimpleRel):
    rev: Optional[PDLVariable | str] = Field(default=None, example='2')
    """
    The revision of the referenced resource.

    """
    resource_key: Optional[PDLVariable | str] = Field(
        default=None, example='f52fe20c-a1fe-4e54-9b78-6bf2ff61b455'
    )
    """
    The resource key for this asset if it exists.

    """


class ModelAssetRef(BaseModel):
    """
    The field that identifies the asset.

    """

    asset: Optional[PDLVariable | ModelRel] = None


class DeploymentResourceEntity(BaseModel):
    """
    The properties specific to `watsonx.ai` deployments.

    """

    base_model_id: Optional[PDLVariable | str] = Field(default=None, example='google/flan-t5-xl')
    """
    The base model that is required for this deployment if this is for a prompt template or a prompt tune for an IBM foundation model.

    """


class Message(BaseModel):
    """
    Optional messages related to the resource.

    """

    level: Optional[PDLVariable | str] = Field(default=None, example='info')
    """
    The level of the message, normally one of `debug`, `info` or `warning`.

    """
    text: Optional[PDLVariable | str] = Field(default=None, example='The deployment is successful')
    """
    The message.

    """


class Inference(BaseModel):
    """
    The details of an inference API.

    """

    url: str
    """
    The inference URL.

    """
    sse: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    This is `true` if the inference API supports SSE streaming.

    """
    uses_serving_name: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    This is `true` if the inference API uses the `serving_name` that was defined in this deployment.

    """


class State(Enum):
    """
    Specifies the current state of the deployment.

    """

    initializing = 'initializing'
    updating = 'updating'
    ready = 'ready'
    failed = 'failed'


class DeploymentStatus(BaseModel):
    """
    Specifies the current status, additional information about the deployment
    and any failure messages in case of deployment failures.

    """

    state: Optional[PDLVariable | State] = Field(default=None, example='ready')
    """
    Specifies the current state of the deployment.

    """
    message: Optional[PDLVariable | Message] = None
    failure: Optional[PDLVariable | ApiErrorResponse] = None
    inference: Optional[PDLVariable | List[Inference]] = Field(
        default=None,
        example=[
            {
                'url': 'https://us-south.ml.cloud.ibm.com/ml/v1/deployments/2cd0bcda-581d-4f04-8028-ec2bc90cc375/text/generation'
            },
            {
                'url': 'https://us-south.ml.cloud.ibm.com/ml/v1/deployments/classification/text/generation',
                'uses_serving_name': True,
            },
            {
                'url': 'https://us-south.ml.cloud.ibm.com/ml/v1/deployments/2cd0bcda-581d-4f04-8028-ec2bc90cc375/text/generation_stream',
                'sse': True,
            },
            {
                'url': 'https://us-south.ml.cloud.ibm.com/ml/v1/deployments/classification/text/generation_stream',
                'sse': True,
                'uses_serving_name': True,
            },
        ],
    )
    """
    The URLs that can be used to submit inference API requests. These URLs will contain the
    `deployment_id` and the `serving_name`, if the `serving_name` was set.

    """


class DeployedAssetType(Enum):
    """
    The type of the deployed model. The possible values are the following:
    1. `prompt_tune` - when a prompt tuned model is deployed.
    2. `foundation_model` - when a prompt template is used on a pre-deployed IBM provided model.

    """

    prompt_tune = 'prompt_tune'
    foundation_model = 'foundation_model'


class DeploymentEntity(DeploymentEntityCommon, ModelAssetRef, DeploymentResourceEntity):
    deployed_asset_type: Optional[PDLVariable | DeployedAssetType] = None
    """
    The type of the deployed model. The possible values are the following:
    1. `prompt_tune` - when a prompt tuned model is deployed.
    2. `foundation_model` - when a prompt template is used on a pre-deployed IBM provided model.

    """
    verbalizer: Optional[PDLVariable | str] = None
    """
    The verbalizer that was used to train this model if the deployment
    has `deployed_asset_type` of `prompt_tune`.

    """
    status: Optional[PDLVariable | DeploymentStatus] = None


class DeploymentResource(BaseModel):
    """
    A deployment resource.
    """

    metadata: Optional[PDLVariable | ResourceMeta] = None
    entity: Optional[PDLVariable | DeploymentEntity] = None


class Stats(BaseModel):
    """
    The stats about deployments for a space.

    """

    space_id: Optional[PDLVariable | str] = Field(
        default=None, example='2fc54cf1-252f-424b-b52d-5cdd98149871'
    )
    """
    An `id` associated with the space.

    """
    total_count: Optional[PDLVariable | float] = None
    """
    The total number of deployments created in a space including `online` and `batch`.

    """
    online_count: Optional[PDLVariable | float] = None
    """
    The number of online deployments created in a space.

    """
    batch_count: Optional[PDLVariable | float] = None
    """
    The number of batch deployments created in a space.

    """


class DeploymentSystemDetails(SystemDetails):
    stats: Optional[PDLVariable | List[Stats]] = None
    """
    The stats about deployments.

    """


class DeploymentSystem(BaseModel):
    """
    System details including warnings.

    """

    system: Optional[PDLVariable | DeploymentSystemDetails] = None


class DeploymentResourceCollection(Pagination):
    resources: Optional[PDLVariable | List[DeploymentResource]] = None
    """
    A list of deployment resources.

    """
    system: Optional[PDLVariable | DeploymentSystem] = None


class EntityRequestSpaceProjectBody(BaseModel):
    """
    The properties that are part of a request that supports spaces and projects.
    Either `space_id` or `project_id` has to be provided and is mandatory.

    """

    name: str = Field(..., example='my-resource')
    """
    The name of the resource.

    """
    project_id: Optional[PDLVariable | 
            constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)
        ] = Field(default=None, example='12ac4cf1-252f-424b-b52d-5cdd9814987f')
    """
    The project that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    space_id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)] = (
        Field(default=None, example='3fc54cf1-252f-424b-b52d-5cdd9814987f')
    )
    """
    The space that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    description: Optional[PDLVariable | str] = Field(
        default=None, example='This is my first resource.'
    )
    """
    A description of the resource.

    """
    tags: Optional[PDLVariable | List[str]] = Field(default=None, example=['t1', 't2'])
    """
    A list of tags for this resource.

    """


class Rel(SimpleRel):
    rev: Optional[PDLVariable | str] = Field(default=None, example='2')
    """
    The revision of the referenced resource.

    """


class AssetRef(BaseModel):
    """
    The field that identifies the asset.

    """

    asset: Optional[PDLVariable | Rel] = None


class DeploymentResourcePrototype(
    EntityRequestSpaceProjectBody,
    DeploymentEntityCommon,
    AssetRef,
    DeploymentResourceEntity,
):
    """
    The deployment request entity (this description is not used).

    """


class Op(Enum):
    """
    The operation to be performed.

    """

    add = 'add'
    remove = 'remove'
    replace = 'replace'


class JsonPatchOperation(BaseModel):
    """
    This model represents an individual patch operation to be performed on an object, as defined by
    [RFC 6902](https://tools.ietf.org/html/rfc6902).

    """

    op: Op
    """
    The operation to be performed.

    """
    path: str
    """
    The pointer that identifies the field that is the target of the operation.

    """
    value: Optional[PDLVariable | str] = None
    """
    The value to be used within the operation.

    """


class JsonPatch(BaseModel):
    """
    See [JSON PATCH RFC 6902](https://tools.ietf.org/html/rfc6902).

    """

    __root__: List[PDLVariable | JsonPatchOperation]
    """
    See [JSON PATCH RFC 6902](https://tools.ietf.org/html/rfc6902).

    """


class CaiKitTextGenProperties(BaseModel):
    """
    Properties that control the model and response that are only supported in caikit.

    """

    typical_p: Optional[PDLVariable | confloat(le=1.0, gt=0.0)] = Field(default=None, example=0.5)
    """
    Local typicality measures how similar the conditional probability of predicting a target
    token next is to the expected conditional probability of predicting a random token next,
    given the partial text already generated. If less than 1, the smallest set of the most
    locally typical tokens with probabilities that add up to typical_p or higher are kept for generation.

    """


class TextGenParameters2(TextGenParameters, CaiKitTextGenProperties):
    pass


class PromptTemplateVariables(BaseModel):
    """
    The template properties if this request refers to a prompt template.

    """

    prompt_variables: Optional[PDLVariable | Dict[str, str]] = None


class DeploymentTextGenProperties(TextGenParameters2, PromptTemplateVariables):
    pass


class DeploymentTextGen(BaseModel):
    input: Optional[PDLVariable | str] = None
    """
    The prompt to generate completions.
    Note: The method tokenizes the input internally.
    It is recommended not to leave any trailing spaces.


    This field is ignored if there is a prompt template.

    """
    parameters: Optional[PDLVariable | DeploymentTextGenProperties] = None
    moderations: Optional[PDLVariable | Moderations] = None


class DeploymentTextGenRequest(DeploymentTextGen):
    """
    A prompt to be used to infer the next tokens.

    """


class BaseModelModel(BaseModel):
    """
    The model id of the base model for this job.

    """

    model_id: Optional[PDLVariable | str] = Field(default=None, example='google/flan-t5-xl')
    """
    The model id of the base model.

    """


class TuningType(Enum):
    """
    Type of Peft (Parameter-Efficient Fine-Tuning) config to build.

    """

    prompt_tuning = 'prompt_tuning'


class InitMethod(Enum):
    """
    The `text` method requires `init_text` to be set.

    """

    random = 'random'
    text = 'text'


class PromptTuning(BaseModel):
    """
    Properties to control the prompt tuning.

    """

    base_model: Optional[PDLVariable | BaseModelModel] = None
    task_id: str = Field(..., example='summarization')
    """
    The task that is targeted for this model.

    """
    tuning_type: Optional[PDLVariable | TuningType] = Field(
        default='prompt_tuning', example='prompt_tuning'
    )
    """
    Type of Peft (Parameter-Efficient Fine-Tuning) config to build.

    """
    num_epochs: Optional[PDLVariable | conint(ge=1, le=50)] = Field(default=20, example=30)
    """
    Number of epochs to tune the prompt vectors, this affects the quality of the trained model.

    """
    learning_rate: Optional[PDLVariable | confloat(ge=0.01, le=0.5)] = Field(default=0.3, example=0.4)
    """
    Learning rate to be used while tuning prompt vectors.

    """
    accumulate_steps: Optional[PDLVariable | conint(ge=1, le=128)] = Field(default=16, example=32)
    """
    Number of steps to be used for gradient accumulation.
    Gradient accumulation refers to a method of collecting gradient for configured number of
    steps instead of updating the model variables at every step and then applying the update
    to model variables. This can be used as a tool to overcome smaller batch size limitation.
    Often also referred in conjunction with "effective batch size".

    """
    verbalizer: Optional[PDLVariable | str] = Field(
        default='Input: {{input}} Output:',
        example='rte { 0 : entailment, 1 : not entailment } {{input}}',
    )
    """
    Verbalizer template to be used for formatting data at train and inference time.
    This template may use brackets to indicate where fields from the data model
    must be rendered.

    """
    batch_size: Optional[PDLVariable | conint(ge=1, le=16)] = Field(default=16, example=10)
    """
    The batch size is a number of samples processed before the model is updated.

    """
    max_input_tokens: Optional[PDLVariable | conint(ge=1, le=256)] = Field(default=256, example=100)
    """
    Maximum length of input tokens being considered.

    """
    max_output_tokens: Optional[PDLVariable | conint(ge=1, le=128)] = Field(default=128, example=100)
    """
    Maximum length of output tokens being predicted.

    """
    init_method: Optional[PDLVariable | InitMethod] = Field(default='random', example='text')
    """
    The `text` method requires `init_text` to be set.

    """
    init_text: Optional[PDLVariable | str] = None
    """
    Initialization text to be used if `init_method` is
    set to `text` otherwise this will be ignored.

    """


class DataConnection(BaseModel):
    """
    Contains a set of fields specific to each connection.
    See here for [details about specifying connections](#datareferences).

    """


class DataSchema(BaseModel):
    """
    The schema of the expected data, see
    [datarecord-metadata-v2-schema](https://raw.githubusercontent.com/elyra-ai/pipeline-schemas/master/common-pipeline/datarecord-metadata/datarecord-metadata-v2-schema.json)
    for the schema definition.

    """

    id: str = Field(..., example='t1')
    """
    An id to identify a schema.

    """
    name: Optional[PDLVariable | str] = Field(default=None, example='Tasks')
    """
    A name for the schema.

    """
    fields: List[PDLVariable | Dict[str, Any]] = Field(
        ..., example=[{'name': 'duration', 'type': 'number'}]
    )
    """
    The fields that describe the data schema.

    """
    type: Optional[PDLVariable | str] = Field(default=None, example='struct')
    """
    The type of the schema, can be ignored or set to `struct` or `DataFrame`.

    """


class Type1(Enum):
    """
    The data source type like `connection_asset` or `data_asset`.
    If the data connection contains just a schema then this field is not required.

    """

    connection_asset = 'connection_asset'
    data_asset = 'data_asset'
    container = 'container'
    url = 'url'


class DataConnectionReference(BaseModel):
    """
    A reference to data with an optional data schema.
    If necessary, it is possible to provide a data connection that contains
    just the data schema.

    """

    id: Optional[PDLVariable | str] = Field(
        default=None, example='8d3682dd-2858-43c9-bfd7-12a79abcfb0c'
    )
    """
    Optional item identification inside a collection.

    """
    type: Type1 = Field(..., example='connection_asset')
    """
    The data source type like `connection_asset` or `data_asset`.
    If the data connection contains just a schema then this field is not required.

    """
    connection: Optional[PDLVariable | DataConnection] = None
    location: Optional[PDLVariable | Dict[str, str]] = None
    schema_: Optional[PDLVariable | DataSchema] = Field(default=None, alias='schema')


class TrainingResourceEntityCommon(BaseModel):
    prompt_tuning: Optional[PDLVariable | PromptTuning] = None
    training_data_references: Optional[PDLVariable | List[DataConnectionReference]] = Field(
        default=None,
        example=[
            {
                'id': 'tune1_data.json',
                'location': {'path': 'tune1_data.json'},
                'type': 'container',
            }
        ],
    )
    """
    Training datasets.

    """
    custom: Optional[PDLVariable | Custom] = None
    auto_update_model: Optional[PDLVariable | bool] = Field(default=False, example=True)
    """
    If set to `true` then the result of the training, if successful, will be uploaded
    to the repository as a model.

    """


class Type2(Enum):
    """
    The data source type like `connection_asset` or `data_asset`.

    """

    connection_asset = 'connection_asset'
    data_asset = 'data_asset'
    container = 'container'
    url = 'url'


class ObjectLocation(BaseModel):
    """
    A reference to data.

    """

    id: Optional[PDLVariable | str] = None
    """
    Item identification inside a collection.

    """
    type: Type2 = Field(..., example='connection_asset')
    """
    The data source type like `connection_asset` or `data_asset`.

    """
    connection: Optional[PDLVariable | DataConnection] = None
    location: Dict[PDLVariable | str, str]


class ResultsReferenceOutput(BaseModel):
    results_reference: ObjectLocation = Field(
        ...,
        example={
            'location': {
                'path': 'results',
                'training': 'results/360c40f7-ac0c-43ca-a95f-1a5421f93b82',
                'training_status': 'results/360c40f7-ac0c-43ca-a95f-1a5421f93b82/training-status.json',
                'assets_path': 'results/360c40f7-ac0c-43ca-a95f-1a5421f93b82/assets',
                'model_request_path': 'results/360c40f7-ac0c-43ca-a95f-1a5421f93b82/assets/c29e7544-dfd0-4427-bc66-20fa6023e2e0/resources/wml_model/request.json',
            },
            'type': 'container',
        },
    )
    """
    The training results. Normally this is specified as `type=container` which means that it is stored in the space or project. Note that the training will add some fields that point to the training status, the model request and the assets.

    The `model_request_path` is the request body that should be used when creating the trained model in the API, if this model is to be deployed. If `auto_update_model` was set to `true` then this file is not needed.

    """


class PromptTuningMetricsContext(BaseModel):
    """
    The context for prompt tuning metrics.

    """

    metrics_location: Optional[PDLVariable | str] = None
    """
    The location where the prompt tuning metrics are stored.

    """


class MetricsContext(BaseModel):
    """
    Provides extra information for this training stage in the context of auto-ml.

    """

    deployment_id: Optional[PDLVariable | str] = None
    """
    The deployment that created the metrics.

    """
    prompt_tuning: Optional[PDLVariable | PromptTuningMetricsContext] = None


class TrainingMetric(BaseModel):
    """
    A metric.

    """

    timestamp: Optional[PDLVariable | datetime] = Field(
        default=None, example='2023-09-22T02:52:03.324Z'
    )
    """
    A timestamp for the metrics.

    """
    iteration: Optional[PDLVariable | int] = Field(default=None, example=0)
    """
    The iteration number.

    """
    ml_metrics: Optional[PDLVariable | Dict[str, float]] = None
    context: Optional[PDLVariable | MetricsContext] = None


class State1(Enum):
    """
    Current state of training.
    """

    queued = 'queued'
    pending = 'pending'
    running = 'running'
    storing = 'storing'
    completed = 'completed'
    failed = 'failed'
    canceled = 'canceled'


class TrainingStatus(BaseModel):
    """
    Status of the training job.

    """

    running_at: Optional[PDLVariable | datetime] = Field(default=None, example='2017-01-30T10:11:12Z')
    """
    Date and Time in which current training state has started.
    """
    completed_at: Optional[PDLVariable | datetime] = Field(
        default=None, example='2017-01-30T10:11:12Z'
    )
    """
    Date and Time in which training had completed.
    """
    state: State1
    """
    Current state of training.
    """
    message: Optional[PDLVariable | Message] = None
    metrics: Optional[PDLVariable | List[TrainingMetric]] = Field(
        default=None,
        example=[
            {
                'iteration': 0,
                'ml_metrics': {'loss': 4.49988},
                'timestamp': '2023-09-22T02:52:03.324Z',
            },
            {
                'iteration': 1,
                'ml_metrics': {'loss': 3.86884},
                'timestamp': '2023-09-22T02:52:03.689Z',
            },
            {
                'iteration': 2,
                'ml_metrics': {'loss': 4.05115},
                'timestamp': '2023-09-22T02:52:04.053Z',
            },
        ],
    )
    """
    Metrics that can be returned by an operation.

    """
    failure: Optional[PDLVariable | ApiErrorResponse] = None


class TrainingStatusOutput(BaseModel):
    """
    Status of the training job.

    """

    status: TrainingStatus


class TrainingResourceEntity(
    TrainingResourceEntityCommon, ResultsReferenceOutput, TrainingStatusOutput
):
    pass


class TrainingResource(BaseModel):
    """
    Training resource.

    """

    metadata: Optional[PDLVariable | ResourceMeta] = Field(
        default=None,
        example={
            'id': '6213cf1-252f-424b-b52d-5cdd9814956c',
            'name': 'my-prompt-training',
            'project_id': '12ac4cf1-252f-424b-b52d-5cdd9814987f',
            'owner': 'guy',
            'created_at': '2023-08-04T13:22:55.289Z',
        },
    )
    entity: Optional[PDLVariable | TrainingResourceEntity] = None


class TrainingResourceCollection(Pagination):
    resources: Optional[PDLVariable | List[TrainingResource]] = None
    """
    The training resources.

    """
    system: Optional[PDLVariable | SystemDetails] = Field(
        default=None,
        example={
            'warnings': [
                {
                    'message': 'This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations.',
                    'id': 'DisclaimerWarning',
                }
            ]
        },
    )


class TrainingResourceDetails(BaseModel):
    """
    The training details required when creating the job.

    """

    name: str = Field(..., example='my-prompt-training')
    """
    The name of the training.

    """
    space_id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)] = (
        Field(default=None, example='3fc54cf1-252f-424b-b52d-5cdd9814987f')
    )
    """
    The space that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    project_id: Optional[PDLVariable | 
            constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)
        ] = Field(default=None, example='12ac4cf1-252f-424b-b52d-5cdd9814987f')
    """
    The project that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    description: Optional[PDLVariable | str] = Field(default=None, example='My prompt training.')
    """
    A description of the training.

    """
    tags: Optional[PDLVariable | List[str]] = Field(default=None, example=['t1', 't2'])
    """
    A list of tags for this resource.

    """


class ResultsReferenceInput(BaseModel):
    results_reference: ObjectLocation = Field(
        ..., example={'location': {'path': 'results'}, 'type': 'container'}
    )
    """
    The training results. Normally this is specified as `type=container` which
    means that it is stored in the space or project.

    """


class TrainingResourcePrototype(
    TrainingResourceDetails, TrainingResourceEntityCommon, ResultsReferenceInput
):
    """
    The `training_data_references` contain the training datasets and the
    `results_reference` the connection where results will be stored.

    """


class EmbeddingReturnOptions(BaseModel):
    """
    The return options for text embeddings.

    """

    input_text: Optional[PDLVariable | bool] = None
    """
    Include the `input` text in each of the `results` documents.

    """


class EmbeddingParameters(BaseModel):
    """
    Parameters for text embedding requests.

    """

    truncate_input_tokens: Optional[PDLVariable | conint(ge=1)] = None
    """
    Represents the maximum number of input tokens accepted.
    This can be used to avoid requests failing due to input being longer than configured limits. If the text is truncated, then it truncates the end of the input (on the right), so the start of the input will remain the same.
    If this value exceeds the `maximum sequence length` (refer to the documentation to find this value for the model) then the call will fail if the total number of tokens exceeds the `maximum sequence length`.

    """
    return_options: Optional[PDLVariable | EmbeddingReturnOptions] = None


class EmbeddingsRequest(BaseModel):
    """
    The text input for a given model to be used to generate the embeddings.

    """

    space_id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)] = (
        Field(default=None, example='3fc54cf1-252f-424b-b52d-5cdd9814987f')
    )
    """
    The space that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    project_id: Optional[PDLVariable | 
            constr(pattern=r'[a-zA-Z0-9-]*', min_length=36, max_length=36)
        ] = Field(default=None, example='12ac4cf1-252f-424b-b52d-5cdd9814987f')
    """
    The project that contains the resource.
    Either `space_id` or `project_id` has to be given.

    """
    model_id: str
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx&audience=wdp).

    """
    inputs: List[PDLVariable | str] = Field(..., max_items=1000)
    """
    The input text.

    """
    parameters: Optional[PDLVariable | EmbeddingParameters] = None


class Embedding(BaseModel):
    """
    The embedding values for a text string. The `input` field is only set if the corresponding `return_option` is set.

    """

    input: Optional[PDLVariable | str] = None
    """
    The text input to the model.

    """
    embedding: List[PDLVariable | float] = Field(..., min_items=0)
    """
    The embedding values.

    """


class EmbeddingsResponseFields(BaseModel):
    """
    The embeddings per input.

    """

    model_id: str
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx&audience=wdp).

    """
    results: List[PDLVariable | Embedding] = Field(..., min_items=0)
    """
    The embedding values for a given text.

    """
    created_at: datetime
    """
    The time when the response was created.

    """
    input_token_count: int
    """
    The number of input tokens that were consumed.

    """


class EmbeddingsResponse(EmbeddingsResponseFields, System):
    pass


class SimilarityResult(BaseModel):
    """
    The similarity results.

    """

    score: float
    """
    A similarity score between the source and target text.

    """


class SimilarityResponseFields(BaseModel):
    """
    The similarity scores per source string.

    """

    model_id: str
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx&audience=wdp).

    """
    results: List[PDLVariable | SimilarityResult] = Field(..., min_items=0)
    """
    The similarity scores.

    """
    created_at: datetime
    """
    The time when the response was created.

    """
    input_token_count: int
    """
    The number of input tokens that were consumed.

    """


class SimilarityResponse(SimilarityResponseFields, System):
    pass


class RerankInput(BaseModel):
    """
    A text to rank.

    """

    text: str
    """
    The text to rank.

    """


class RerankedResults(BaseModel):
    """
    The ranking score for the input.

    """

    input: Optional[PDLVariable | RerankInput] = None
    score: float
    """
    The score of the input.

    """


class RerankResponseFields(BaseModel):
    """
    The ranked results.

    """

    model_id: str
    """
    The `id` of the model to be used for this request.
    Please refer to the [list of models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx&audience=wdp).

    """
    query: Optional[PDLVariable | str] = None
    """
    The rank query.

    """
    results: List[PDLVariable | RerankedResults] = Field(..., min_items=0)
    """
    The ranked results.

    """
    created_at: datetime
    """
    The time when the response was created.

    """
    input_token_count: int
    """
    The number of input tokens that were consumed.

    """


class RerankResponse(RerankResponseFields, System):
    pass


class ModerationInputProperties(BaseModel):
    """
    The properties for the moderation. Each type of moderation
    may have additional properties that are specific to that moderation.

    """

    input: Optional[PDLVariable | TextModeration] = None


class ModerationHapInputProperties(ModerationInputProperties, HapProperties):
    pass


class ModerationPiiInputProperties(ModerationInputProperties, PiiProperties):
    pass


class CommonPatchRequestHelper(BaseModel):
    """
    The common fields that can be patched.
    This is a helper for `cpdctl`.

    """

    tags: Optional[PDLVariable | List[str]] = Field(default=None, example=['t1', 't2'])
    """
    A list of tags for this resource.

    """
    name: Optional[PDLVariable | str] = Field(default=None, example='my-resource')
    """
    The name of the resource.

    """
    description: Optional[PDLVariable | str] = Field(
        default=None, example='This is my first resource.'
    )
    """
    A description of the resource.

    """
    custom: Optional[PDLVariable | Custom] = None


class InputMode(Enum):
    """
    Input mode in use for the prompt
    """

    structured = 'structured'
    freeform = 'freeform'
    chat = 'chat'
    detached = 'detached'


class ModelVersion(BaseModel):
    number: Optional[PDLVariable | 
            constr(
                pattern=r'^(0|[1-9]\d*)\.(0|[1-9]\d*)\.(0|[1-9]\d*)(?:-((?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\.(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\+([0-9a-zA-Z-]+(?:\.[0-9a-zA-Z-]+)*))?$'
            )
        ] = Field(default=None, example='2.0.0-rc.7')
    """
    User provided semantic version for tracking in IBM AI Factsheets
    """
    tag: Optional[PDLVariable | constr(pattern=r'.*')] = Field(default=None, example='tag')
    """
    User provived tag.
    """
    description: Optional[PDLVariable | constr(pattern=r'.*')] = Field(
        default=None, example='Description of the model version.'
    )
    """
    Description of the version.
    """


class InputMode2(Enum):
    """
    Input mode in use for the prompt
    """

    structured = 'structured'
    freeform = 'freeform'


class InputMode3(Enum):
    """
    Input mode in use for the prompt
    """

    structured = 'structured'
    freeform = 'freeform'
    chat = 'chat'


class Result1(BaseModel):
    id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='1c29d9a1-9ba6-422d-aa39-517b26adc147'
    )
    """
    The prompt entry's ID
    """
    name: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='Name of an entry'
    )
    """
    The prompt entry's name
    """
    description: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='Description of an entry'
    )
    """
    The prompt entry's description
    """
    created_at: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)
    """
    The prompt entry's create time in millis
    """


class WxPromptSessionEntryList(BaseModel):
    class Config:
        extra = Extra.forbid

    results: Optional[PDLVariable | List[Result1]] = None
    bookmark: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = None


class ModelParameters(BaseModel):
    decoding_method: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = None
    max_new_tokens: Optional[PDLVariable | int] = None
    min_new_tokens: Optional[PDLVariable | int] = None
    random_seed: Optional[PDLVariable | int] = None
    stop_sequences: Optional[PDLVariable | List[constr(pattern=r'[a-zA-Z0-9-]*')]] = None
    temperature: Optional[PDLVariable | float] = None
    top_k: Optional[PDLVariable | float] = None
    top_p: Optional[PDLVariable | float] = None
    repetition_penalty: Optional[PDLVariable | float] = None


class ExternalModel(BaseModel):
    class Config:
        extra = Extra.forbid

    name: constr(pattern=r'.*')
    url: constr(pattern=r'.*')


class ExternalPromptAdditionalInformation1(BaseModel):
    class Config:
        extra = Extra.forbid

    key: Optional[PDLVariable | constr(pattern=r'^[\s\S]{0,250}$')] = None


class Type3(Enum):
    question = 'question'
    answer = 'answer'


class Status(Enum):
    ready = 'ready'
    error = 'error'


class ChatItem(BaseModel):
    class Config:
        extra = Extra.forbid

    type: Optional[PDLVariable | Type3] = None
    content: Optional[PDLVariable | constr(pattern=r'.*')] = Field(default=None, example='Some text')
    status: Optional[PDLVariable | Status] = None
    timestamp: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)


class PromptData(BaseModel):
    class Config:
        extra = Extra.forbid

    instruction: Optional[PDLVariable | constr(pattern=r'[\s\S]*')] = None
    input_prefix: Optional[PDLVariable | constr(pattern=r'[\s\S]*')] = None
    output_prefix: Optional[PDLVariable | constr(pattern=r'[\s\S]*')] = None
    examples: Optional[PDLVariable | List[List[constr(pattern=r'^[\s\S]*')]]] = []


class LockType(Enum):
    """
    Lock type: 'edit' for working on prompts/templates or 'governance'. Can only be supplied in PUT /lock requests.
    """

    edit = 'edit'
    governance = 'governance'


class PromptLock(BaseModel):
    class Config:
        extra = Extra.forbid

    locked: bool
    """
    True if the prompt is currently locked.
    """
    lock_type: Optional[PDLVariable | LockType] = None
    """
    Lock type: 'edit' for working on prompts/templates or 'governance'. Can only be supplied in PUT /lock requests.
    """
    locked_by: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='IBMid-000000YYY0'
    )
    """
    Locked by is computed by the server and shouldn't be passed.
    """


class PromptVariable(BaseModel):
    pass

    class Config:
        extra = Extra.allow


class NotebookMetadata(BaseModel):
    """
    Metadata of a notebook.
    """

    name: Optional[PDLVariable | str] = Field(default=None, example='my notebook')
    """
    The name of the notebook.
    """
    description: Optional[PDLVariable | str] = Field(default=None, example='this is my notebook')
    """
    A more verbose description.
    """
    asset_type: Optional[PDLVariable | str] = Field(default=None, example='notebook')
    """
    Asset type, always "notebook".
    """
    created: Optional[PDLVariable | int] = Field(default=None, example=1540471021134)
    """
    Creation date, ms since epoch.
    """
    created_at: Optional[PDLVariable | str] = Field(default=None, example='2018-10-25T12:37:01Z')
    """
    Creation date, ISO format.
    """
    owner_id: Optional[PDLVariable | str] = Field(default=None, example='IBMid-310000SG2Y')
    """
    IAM ID of the asset's owner.
    """
    catalog_id: Optional[PDLVariable | str] = Field(
        default=None, example='463cb8d8-8480-4a98-b75a-f7443b7d0af9'
    )
    """
    UUID of the asset's catalog.
    """
    asset_id: Optional[PDLVariable | str] = Field(
        default=None, example='41d09a9a-f771-48a2-9534-50c0c622356d'
    )
    """
    UUID of the asset.
    """


class NotebookMetadataInProject(NotebookMetadata):
    """
    Metadata of a notebook in a project.
    """

    project_id: str = Field(..., example='b275be5f-10ff-47ee-bfc9-63f1ce5addbf')
    """
    UUID of the asset's project.
    """


class NotebookResourceMetadata(BaseModel):
    """
    Metadata of notebook info returned in a listing.
    """

    guid: Optional[PDLVariable | str] = Field(
        default=None, example='299993bf-9a42-48ae-aadd-1336f31d5556'
    )
    """
    UUID of the notebook.
    """
    url: Optional[PDLVariable | str] = Field(
        default=None, example='/v2/notebooks/299993bf-9a42-48ae-aadd-1336f31d5556'
    )
    """
    URL of the notebook.
    """


class NotebookResourceEntityAsset(BaseModel):
    """
    Asset API asset description returned with a notebook listing.
    """

    asset_id: Optional[PDLVariable | str] = Field(
        default=None, example='41d09a9a-f771-48a2-9534-50c0c622356d'
    )
    """
    The UUID of the asset.
    """
    asset_type: Optional[PDLVariable | str] = Field(default=None, example='notebook')
    """
    The asset type. Always "notebook".
    """
    created: Optional[PDLVariable | int] = Field(default=None, example=1540471021134)
    """
    Timestamp of the creation date, ms since epoch.
    """
    created_at: Optional[PDLVariable | str] = Field(default=None, example='2018-10-25T12:37:01Z')
    """
    Date the asset was created, ISO format.
    """
    catalog_id: Optional[PDLVariable | str] = Field(
        default=None, example='463cb8d8-8480-4a98-b75a-f7443b7d0af9'
    )
    """
    The asset catalog ID.
    """
    project_id: Optional[PDLVariable | str] = Field(
        default=None, example='b275be5f-10ff-47ee-bfc9-63f1ce5addbf'
    )
    """
    The project the notebook belongs to.
    """
    version: Optional[PDLVariable | int] = Field(default=None, example=2)
    """
    Version of the asset.
    """
    href: Optional[PDLVariable | str] = Field(
        default=None,
        example='/v2/assets/299993bf-9a42-48ae-aadd-1336f31d5556?project_id=850d08c4-31f1-4722-a7ef-eeefd796e995',
    )
    """
    The asset URL.
    """


class NotebookResourceEntityRuntime(BaseModel):
    """
    Runtime info returned with a notebook listing.
    """

    spark_monitoring_enabled: Optional[PDLVariable | bool] = Field(default=None, example=True)
    """
    If Spark monitoring is enabled.
    """
    environment: Optional[PDLVariable | str] = Field(
        default=None, example='conda4x16-850d08c4-31f1-4722-a7ef-eeefd796e995'
    )
    """
    UUID of the environment of the notebook.
    """


class NotebookCopyBody(BaseModel):
    """
    Payload for copying a notebook.
    """

    name: str = Field(..., example='my notebook')
    """
    The name of the new notebook.
    """
    source_guid: str = Field(..., example='ca3c0e27-46ca-83d4-a646-d49b11c14de9')
    """
    The guid of the notebook to be copied.
    """


class NotebookRevertBody(BaseModel):
    """
    Payload for a request to revert to a specific notebook version.
    """

    source: str = Field(..., example='ca3c0e27-46ca-83d4-a646-d49b11c14de9')
    """
    The guid of the notebook version.
    """


class NotebookOrigin(BaseModel):
    """
    The notebook origin.
    """

    type: Optional[PDLVariable | str] = Field(default=None, example='blank')
    """
    The orgin type of the notebook, either blank, file or url.
    """


class NotebookOriginFromSource(BaseModel):
    """
    The origin of a notebook from a source.
    """

    type: Optional[PDLVariable | str] = Field(default=None, example='notebook')
    """
    The orgin type of the notebook, either blank, file or url.
    """
    guid: Optional[PDLVariable | str] = Field(
        default=None, example='ca3c0e27-46ca-83d4-a646-d49b11c14de9'
    )
    """
    The guid of the source file
    """


class NotebookRuntime(BaseModel):
    """
    A notebook runtime.
    """

    environment: str = Field(
        ..., example='conda4x16-d46ca0e27-a646-4de9-a646-9b113c183d4'
    )
    """
    The guid of the environment on which the notebook runs.
    """
    spark_monitoring_enabled: Optional[PDLVariable | bool] = None
    """
    Spark monitoring enabled or not.
    """


class NotebookKernel(BaseModel):
    """
    A notebook kernel.
    """

    display_name: Optional[PDLVariable | str] = Field(default=None, example='Python 3.9 with Spark')
    """
    The display name of the environment kernel.
    """
    name: Optional[PDLVariable | str] = Field(default=None, example='python3')
    """
    The name of the environment kernel.
    """
    language: Optional[PDLVariable | str] = Field(default=None, example='python3')
    """
    The language of the environment kernel.
    """


class NotebookListBody(BaseModel):
    """
    Payload for a notebook list request.
    """

    notebooks: Optional[PDLVariable | List[str]] = None
    """
    The list of notebooks whose details will be retrieved.
    """


class NotebookUpdateBody(BaseModel):
    """
    Payload for a notebook update request.
    """

    environment: Optional[PDLVariable | str] = Field(
        default=None, example='d46ca0e27-a646-4de9-a646-9b113c183d4'
    )
    """
    The guid of the environment on which the notebook runs.
    """
    spark_monitoring_enabled: Optional[PDLVariable | bool] = Field(default=None, example=False)
    """
    Spark monitoring enabled or not.
    """
    kernel: Optional[PDLVariable | NotebookKernel] = None


class NotebookVersionMetadata(BaseModel):
    """
    Notebook version metadata.
    """

    guid: Optional[PDLVariable | str] = Field(
        default=None, example='19d63b6b-81a1-4c05-bad2-36a2957bd6d0'
    )
    """
    The guid of the version.
    """
    url: Optional[PDLVariable | str] = Field(
        default=None,
        example='v2/notebooks/a528b427-d1cd-4039-8ddc-04203c2521e2/versions/1a1329e0-fd05-409a-8411-52db106e2142',
    )
    """
    The URL of the version.
    """
    created_at: Optional[PDLVariable | int] = Field(default=None, example=1543681714106)
    """
    The creation timestamp in UTC millisecond since UNIX Epoch time.
    """


class NotebookVersionEntity(BaseModel):
    """
    A notebook version entity.
    """

    master_notebook_guid: Optional[PDLVariable | str] = Field(
        default=None, example='a528b427-d1cd-4039-8ddc-04203c2521e2'
    )
    """
    The guid of the versioned notebook.
    """
    created_by_iui: Optional[PDLVariable | str] = Field(default=None, example='IBMid-123456ABCD')
    """
    The IUI of the user that has created the version.
    """
    file_reference: Optional[PDLVariable | str] = Field(
        default=None,
        example='myproject-donotdelete-pr-6p65nym92j1bv0/notebooks/GPU_ENVIRONMENT_DEFAULT_GBUXVKHH_version_1543781324804.ipynb',
    )
    """
    The file reference in the corresponding COS.
    """
    rev_id: Optional[PDLVariable | int] = Field(default=None, example=1)
    """
    The revision id of the notebook.
    """


class NotebookVersionEntityInProject(NotebookVersionEntity):
    """
    A notebook version entity in a project.
    """

    project_id: str = Field(..., example='0f7c1111-a79d-45b2-9699-d4950e742964')
    """
    The guid of the project.
    """


class Type4(Enum):
    """
    The type of the problematic field.
    """

    field = 'field'
    query = 'query'
    header = 'header'


class ErrorTarget(BaseModel):
    """
    The target of the error.
    """

    type: Type4
    """
    The type of the problematic field.
    """
    name: str
    """
    The name of the problematic field.
    """


class DeploymentResourcePatch(CommonPatchRequestHelper):
    asset: Optional[PDLVariable | Rel] = None


class Prompt(BaseModel):
    class Config:
        extra = Extra.forbid

    input: Optional[PDLVariable | List[List[constr(pattern=r'[a-zA-Z0-9-]*')]]] = []
    model_id: constr(pattern=r'[a-zA-Z0-9-//]*') = Field(
        ..., example='ibm/granite-13b-chat-v2'
    )
    model_parameters: Optional[PDLVariable | ModelParameters] = None
    data: PromptData
    system_prompt: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = None
    chat_items: Optional[PDLVariable | List[ChatItem]] = None


class ExternalPrompt(BaseModel):
    class Config:
        extra = Extra.forbid

    url: constr(pattern=r'.*')
    additional_information: Optional[PDLVariable | 
            List[List[ExternalPromptAdditionalInformation1]]
        ] = Field(default=None, min_items=1)


class ExternalInformation(BaseModel):
    class Config:
        extra = Extra.forbid

    external_prompt_id: constr(pattern=r'.*')
    external_model_id: constr(pattern=r'.*')
    external_model_provider: constr(pattern=r'.*')
    external_prompt: Optional[PDLVariable | ExternalPrompt] = None
    external_model: Optional[PDLVariable | ExternalModel] = None


class WxPromptInputRequest(BaseModel):
    class Config:
        extra = Extra.forbid

    input: Optional[PDLVariable | constr(pattern=r'.*')] = Field(
        default=None, example='Some text with variables.'
    )
    """
    Override input string that will be used to generate the response. The string can contain template parameters.
    """
    prompt_variables: Optional[PDLVariable | Dict[str, constr(pattern=r'[a-zA-Z0-9-]*')]] = None
    """
    Supply only to replace placeholders. Object content must be key:value pairs where the 'key' is the parameter to replace and 'value' is the value to use.
    """


class NotebookEntityDefinition(BaseModel):
    """
    Definition part of a notebook entity.
    """

    kernel: Optional[PDLVariable | NotebookKernel] = None
    originates_from: Optional[PDLVariable | NotebookOrigin] = None


class NotebookEntityDefinitionForCopy(BaseModel):
    """
    Definition part of a notebook entity copied from a source.
    """

    kernel: Optional[PDLVariable | NotebookKernel] = None
    originates_from: Optional[PDLVariable | NotebookOriginFromSource] = None


class NotebookResourceEntity(BaseModel):
    """
    Entity of notebook info returned in a listing.
    """

    asset: Optional[PDLVariable | NotebookResourceEntityAsset] = None
    runtime: Optional[PDLVariable | NotebookResourceEntityRuntime] = None


class NotebookCreateBodyGeneral(BaseModel):
    """
    Payload for creating a notebook.
    """

    name: str = Field(..., example='my notebook')
    """
    The name of the new notebook.
    """
    description: Optional[PDLVariable | str] = Field(default=None, example='this is my notebook')
    """
    A more verbose description of the notebook.
    """
    file_reference: str = Field(..., example='notebook/my_notebook.ipynb')
    """
    The reference to the file in the object storage.
    """
    originates_from: Optional[PDLVariable | NotebookOrigin] = None
    runtime: NotebookRuntime
    kernel: Optional[PDLVariable | NotebookKernel] = None


class NotebookVersionInProject(BaseModel):
    """
    A notebook version in a project.
    """

    metadata: Optional[PDLVariable | NotebookVersionMetadata] = None
    entity: Optional[PDLVariable | NotebookVersionEntityInProject] = None


class NotebookVersionsListInProject(BaseModel):
    """
    A list of notebook versions in a project.
    """

    total_results: int = Field(..., example=1)
    """
    The number of items in the resources array.
    """
    resources: List[PDLVariable | NotebookVersionInProject]
    """
    An array of notebook versions.
    """


class Error(BaseModel):
    """
    An error object.
    """

    code: str
    """
    The code describing the error.
    """
    message: str
    """
    The detailed information about the error.
    """
    target: Optional[PDLVariable | ErrorTarget] = None


class WxPromptPatch(BaseModel):
    class Config:
        extra = Extra.forbid

    id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='1c29d9a1-9ba6-422d-aa39-517b26adc147'
    )
    """
    The prompt's id. This value cannot be set. It is returned in responses only.
    """
    name: constr(pattern=r'[a-zA-Z0-9-]*') = Field(..., example='My Prompt')
    """
    Name used to display the prompt.
    """
    description: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='My First Prompt'
    )
    """
    An optional description for the prompt.
    """
    task_ids: Optional[PDLVariable | List[constr(pattern=r'[a-zA-Z0-9-]*')]] = Field(
        default=None, max_items=1, min_items=1
    )
    governance_tracked: Optional[PDLVariable | bool] = None
    model_version: Optional[PDLVariable | ModelVersion] = None
    prompt_variables: Optional[PDLVariable | Dict[str, PromptVariable]] = None
    input_mode: Optional[PDLVariable | InputMode2] = None
    """
    Input mode in use for the prompt
    """
    prompt: Prompt


class WxPromptSessionEntry(BaseModel):
    class Config:
        extra = Extra.forbid

    id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='1c29d9a1-9ba6-422d-aa39-517b26adc147'
    )
    """
    The prompt's id. This value cannot be set. It is returned in responses only.
    """
    name: constr(pattern=r'[a-zA-Z0-9-]*') = Field(..., example='My Prompt')
    """
    Name used to display the prompt.
    """
    description: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='My First Prompt'
    )
    """
    An optional description for the prompt.
    """
    prompt_variables: Optional[PDLVariable | Dict[str, PromptVariable]] = None
    is_template: Optional[PDLVariable | bool] = None
    created_at: int = Field(..., example=1711504485261)
    """
    Time the prompt was created.
    """
    input_mode: Optional[PDLVariable | InputMode3] = None
    """
    Input mode in use for the prompt
    """
    prompt: Prompt


class PromptWithExternal(BaseModel):
    class Config:
        extra = Extra.forbid

    input: Optional[PDLVariable | List[List[constr(pattern=r'[a-zA-Z0-9-]*')]]] = []
    model_id: constr(pattern=r'[a-zA-Z0-9-//]*') = Field(
        ..., example='ibm/granite-13b-chat-v2'
    )
    model_parameters: Optional[PDLVariable | ModelParameters] = None
    data: PromptData
    system_prompt: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = None
    chat_items: Optional[PDLVariable | List[ChatItem]] = None
    external_information: Optional[PDLVariable | ExternalInformation] = None


class NotebookEntity(BaseModel):
    """
    Entity of a notebook.
    """

    notebook: Optional[PDLVariable | NotebookEntityDefinition] = None
    runtime: Optional[PDLVariable | NotebookRuntime] = None
    href: Optional[PDLVariable | str] = Field(
        default=None,
        example='/v2/assets/41d09a9a-f771-48a2-9534-50c0c622356d?project_id=b275be5f-10ff-47ee-bfc9-63f1ce5addbf',
    )
    """
    Full URI of the notebook.
    """


class NotebookEntityForCopy(BaseModel):
    """
    Entity of a notebook copied from a source.
    """

    notebook: Optional[PDLVariable | NotebookEntityDefinitionForCopy] = None
    runtime: Optional[PDLVariable | NotebookRuntime] = None
    href: Optional[PDLVariable | str] = Field(
        default=None,
        example='/v2/assets/41d09a9a-f771-48a2-9534-50c0c622356d?project_id=b275be5f-10ff-47ee-bfc9-63f1ce5addbf',
    )
    """
    Full URI of the notebook.
    """


class NotebookResource(BaseModel):
    """
    Notebook info returned in a listing.
    """

    metadata: Optional[PDLVariable | NotebookResourceMetadata] = None
    entity: Optional[PDLVariable | NotebookResourceEntity] = None


class NotebookCreateBodyInProject(NotebookCreateBodyGeneral):
    """
    Payload for creating a notebook in a project.
    """

    project: str = Field(..., example='92ae0e27-9b11-4de9-a646-d46ca3c183d4')
    """
    The guid of the project in which to create the notebook.
    """


class ErrorResponse(BaseModel):
    """
    An error response.
    """

    trace: str
    """
    The trace ID used in logs.
    """
    errors: List[PDLVariable | Error]
    """
    The error objects.
    """


class WxPromptResponse(BaseModel):
    class Config:
        extra = Extra.forbid

    id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='1c29d9a1-9ba6-422d-aa39-517b26adc147'
    )
    """
    The prompt's id. This value cannot be set. It is returned in responses only.
    """
    name: constr(pattern=r'[a-zA-Z0-9-]*') = Field(..., example='My Prompt')
    """
    Name used to display the prompt.
    """
    description: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='My First Prompt'
    )
    """
    An optional description for the prompt.
    """
    created_at: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)
    """
    Time the prompt was created.
    """
    created_by: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='IBMid-000000YYY0'
    )
    """
    The ID of the original prompt creator.
    """
    last_updated_at: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)
    """
    Time the prompt was updated.
    """
    last_updated_by: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='IBMid-000000YYY0'
    )
    """
    The ID of the last user that modifed the prompt.
    """
    task_ids: Optional[PDLVariable | List[constr(pattern=r'[a-zA-Z0-9-]*')]] = Field(
        default=None, max_items=1, min_items=1
    )
    governance_tracked: Optional[PDLVariable | bool] = None
    lock: Optional[PDLVariable | PromptLock] = None
    input_mode: Optional[PDLVariable | InputMode] = None
    """
    Input mode in use for the prompt
    """
    model_version: Optional[PDLVariable | ModelVersion] = None
    prompt_variables: Optional[PDLVariable | Dict[str, PromptVariable]] = None
    is_template: Optional[PDLVariable | bool] = None
    resource_key: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = None
    prompt: PromptWithExternal


class WxPromptPost(BaseModel):
    class Config:
        extra = Extra.forbid

    name: constr(pattern=r'[a-zA-Z0-9-]*') = Field(..., example='My Prompt')
    """
    Name used to display the prompt.
    """
    description: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='My First Prompt'
    )
    """
    An optional description for the prompt.
    """
    created_at: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)
    """
    Time the prompt was created.
    """
    task_ids: Optional[PDLVariable | List[constr(pattern=r'[a-zA-Z0-9-]*')]] = Field(
        default=None, max_items=1, min_items=1
    )
    lock: Optional[PDLVariable | PromptLock] = None
    model_version: Optional[PDLVariable | ModelVersion] = None
    prompt_variables: Optional[PDLVariable | Dict[str, PromptVariable]] = None
    input_mode: Optional[PDLVariable | InputMode] = None
    """
    Input mode in use for the prompt
    """
    prompt: PromptWithExternal


class WxPromptSession(BaseModel):
    class Config:
        extra = Extra.forbid

    id: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]{32}')] = Field(
        default=None, example='1c29d9a1-9ba6-422d-aa39-517b26adc147'
    )
    """
    The prompt session's id. This value cannot be set. It is returned in responses only.
    """
    name: constr(pattern=r'^.{0,100}$') = Field(..., example='Session 1')
    """
    Name used to display the prompt session.
    """
    description: Optional[PDLVariable | constr(pattern=r'^[\s\S]{0,250}')] = Field(
        default=None, example='My First Prompt Session'
    )
    """
    An optional description for the prompt session.
    """
    created_at: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)
    """
    Time the session was created.
    """
    created_by: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='IBMid-000000YYY0'
    )
    """
    The ID of the original session creator.
    """
    last_updated_at: Optional[PDLVariable | int] = Field(default=None, example=1711504485261)
    """
    Time the session was updated.
    """
    last_updated_by: Optional[PDLVariable | constr(pattern=r'[a-zA-Z0-9-]*')] = Field(
        default=None, example='IBMid-000000YYY0'
    )
    """
    The ID of the last user that modifed the session.
    """
    lock: Optional[PDLVariable | PromptLock] = None
    prompts: Optional[PDLVariable | List[WxPromptSessionEntry]] = Field(
        default=None, max_items=50, min_items=0
    )


class NotebookInProject(BaseModel):
    """
    Notebook information in a project as returned by a GET request.
    """

    metadata: Optional[PDLVariable | NotebookMetadataInProject] = None
    entity: Optional[PDLVariable | NotebookEntity] = None


class NotebooksResourceList(BaseModel):
    """
    A list of notebook info as returned by a list query.
    """

    total_results: int = Field(..., example=1)
    """
    The number of items in the resources list.
    """
    resources: List[PDLVariable | NotebookResource]
    """
    An array of notebooks.
    """


class Notebook(BaseModel):
    """
    Notebook information as returned by a GET request.
    """

    metadata: Optional[PDLVariable | NotebookMetadata] = None
    entity: Optional[PDLVariable | NotebookEntity] = None


class NotebookForCopy(BaseModel):
    """
    Information of a copied notebook as returned by a GET request.
    """

    metadata: Optional[PDLVariable | NotebookMetadataInProject] = None
    entity: Optional[PDLVariable | NotebookEntityForCopy] = None
